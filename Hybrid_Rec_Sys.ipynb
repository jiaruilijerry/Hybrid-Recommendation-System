{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZyJ0k1ji6oH"
      },
      "source": [
        "**1. Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0mnDfe8i8Ih"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    DataCollatorForWholeWordMask,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TnhNz4vi9S1"
      },
      "source": [
        "**2. Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCrZd92gjDDu",
        "outputId": "4ccf1949-d892-4627-e59c-d00a0999e5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utENJtrgjFqJ"
      },
      "outputs": [],
      "source": [
        "train_users_pth = '/content/drive/MyDrive/CMSC472 FP/Data/MINDsmall_train/behaviors.tsv'\n",
        "train_news_pth =  '/content/drive/MyDrive/CMSC472 FP/Data/MINDsmall_train/news.tsv'\n",
        "dev_news_pth = '/content/drive/MyDrive/CMSC472 FP/Data/MINDsmall_dev/news.tsv'\n",
        "\n",
        "users = pd.read_csv(train_users_pth, delimiter='\\t', header=None)\n",
        "news = pd.read_csv(train_news_pth, delimiter='\\t', header=None)\n",
        "dev = pd.read_csv(dev_news_pth, delimiter='\\t', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrutcLP0jRvP"
      },
      "source": [
        "**Preprocess Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcXBtDi-jT5Y"
      },
      "outputs": [],
      "source": [
        "def process_users(users):\n",
        "    result = users.drop(columns=[2])\n",
        "    result[3] = result[3].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
        "    result[4] = result[4].apply(lambda x: [item.split('-') for item in x.split()] if isinstance(x, str) else [])\n",
        "    result.columns = [\"impression_id\", \"user_id\", \"history\", \"impressions\"]\n",
        "    # print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0Zur1uJjVwd"
      },
      "outputs": [],
      "source": [
        "def extract_entities(col):\n",
        "    return col.apply(\n",
        "        lambda x: ' '.join([ent['Label'] for ent in json.loads(x)]) if isinstance(x, str) and x != '[]' else ''\n",
        "    )\n",
        "\n",
        "def process_news(news):\n",
        "    result = news.drop(columns=[5])\n",
        "    title_labels = extract_entities(result[6])\n",
        "    abstract_labels = extract_entities(result[7])\n",
        "    result[3] = result[3].fillna('')\n",
        "    result[4] = result[4].fillna('')\n",
        "    result['news_info'] = 'Category: ' + result[1] + ' SubCategory: ' + result[2] + ' Label1: ' + title_labels + ' Label2: ' + abstract_labels + ' Title: ' + result[3] + ' Abstract: ' + result[4]\n",
        "    result = result.drop(columns=[3])\n",
        "    result = result.drop(columns=[4])\n",
        "    result = result.drop(columns=[6])\n",
        "    result = result.drop(columns=[7])\n",
        "    result.columns = [\"news_id\", \"category\", \"sub_category\", \"news_info\"]\n",
        "\n",
        "    # print(result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epzvs1DrjZZi"
      },
      "source": [
        "Users history and impressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMsTroXjbqD"
      },
      "outputs": [],
      "source": [
        "train_users = process_users(users)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGHcO81cjg_4"
      },
      "source": [
        "Preprocessing news title, abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PljfzCDqjhgQ"
      },
      "outputs": [],
      "source": [
        "train_news = process_news(news)\n",
        "dev_news = process_news(dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7aEUSCpjkAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3277a5fc-ae23-4f20-addf-bacb8457ea06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N55528\n"
          ]
        }
      ],
      "source": [
        "train_sentences = train_news['news_info'].apply(str.strip).tolist()\n",
        "dev_sentences = dev_news['news_info'].apply(str.strip).tolist()\n",
        "# print(dev_news['sub_category'][0])\n",
        "# print(dev_news['news_info'][0])\n",
        "print(dev_news['news_id'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjB6s_dejlUs",
        "outputId": "6a9fa99e-5ae0-4aa0-b096-ff30cbc0a3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42416\n"
          ]
        }
      ],
      "source": [
        "print(len(dev_sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IehgbsvajnjT",
        "outputId": "a7b7a881-87bc-4559-a2b1-724def1f5dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31331\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for sentence in dev_sentences:\n",
        "    if len(sentence) < 512:\n",
        "        count += 1\n",
        "\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_5Vk3adjs1D"
      },
      "source": [
        "**Model Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoZvin4rjvlF"
      },
      "outputs": [],
      "source": [
        "model_name = 'roberta-base'\n",
        "per_device_train_batch_size = 16\n",
        "\n",
        "save_steps = 1000  # Save model every 1k steps\n",
        "num_train_epochs = 3  # Number of epochs\n",
        "use_fp16 = False  # Set to True, if your GPU supports FP16 operations\n",
        "max_length = 512  # Max length for a text input\n",
        "do_whole_word_mask = True  # If set to true, whole words are masked\n",
        "mlm_prob = 0.15  # Probability that a word is replaced by a [MASK] token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "QCPxPO6zjzU1",
        "outputId": "3e6cc8e5-8bbd-46cc-b537-02dc97778209"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_name' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2509fc1b1c36>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
          ]
        }
      ],
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6kcQZ7EkHd3"
      },
      "source": [
        "**Save Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7DJNwmyj3N3",
        "outputId": "81e98410-637c-4787-b586-43cc1d33f0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save checkpoints to: /content/drive/MyDrive/Hybrid_Rec_Sys/bert-base-uncased-2024-11-29_05-49-59\n"
          ]
        }
      ],
      "source": [
        "output_dir = \"/content/drive/MyDrive/Hybrid_Rec_Sys/bert-base-uncased-2024-11-29_05-49-59\"\n",
        "print(\"Save checkpoints to:\", output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPqz0PtkKQX"
      },
      "source": [
        "**Training and Validation Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRnx8O1Mj8Pa"
      },
      "outputs": [],
      "source": [
        "class TokenizedSentencesDataset:\n",
        "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = sentences\n",
        "        self.max_length = max_length\n",
        "        self.cache_tokenization = cache_tokenization\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.cache_tokenization:\n",
        "            return self.tokenizer(\n",
        "                self.sentences[item],\n",
        "                add_special_tokens=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_special_tokens_mask=True,\n",
        "            )\n",
        "\n",
        "        if isinstance(self.sentences[item], str):\n",
        "            self.sentences[item] = self.tokenizer(\n",
        "                self.sentences[item],\n",
        "                add_special_tokens=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_special_tokens_mask=True,\n",
        "            )\n",
        "        return self.sentences[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_R4uSp-kAet"
      },
      "outputs": [],
      "source": [
        "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
        "# dev_sentences = []\n",
        "dev_dataset = (\n",
        "    TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True)\n",
        "    if len(dev_sentences) > 0\n",
        "    else None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dh6pqBNkCLZ"
      },
      "outputs": [],
      "source": [
        "if do_whole_word_mask:\n",
        "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "else:\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctIHy8XekU5g"
      },
      "source": [
        "**Trainer Args**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPi9gvhPkDnb",
        "outputId": "277d8cd3-2708-438f-8f64-69461841d9f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args_list = [TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    eval_steps=save_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=save_steps,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=use_fp16,\n",
        "    learning_rate=7e-05\n",
        "), TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    eval_steps=save_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=save_steps,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=use_fp16,\n",
        "    learning_rate=3e-05)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab4e_uccwtTd"
      },
      "outputs": [],
      "source": [
        "trainer_list = []\n",
        "for training_args in training_args_list:\n",
        "    trainer_list.append(Trainer(\n",
        "        model=model, args=training_args, data_collator=data_collator, train_dataset=train_dataset, eval_dataset=dev_dataset\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayW21xTdw1d3"
      },
      "source": [
        "Save Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCutx0Fzwy7N",
        "outputId": "0cb48cef-d335-4402-89db-e58473acc059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save tokenizer to: /content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/vocab.json',\n",
              " '/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/merges.txt',\n",
              " '/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/added_tokens.json',\n",
              " '/content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37/tokenizer.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Save tokenizer to:\", output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bmGQBDakhQL"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "pAhslM7ikpzY",
        "outputId": "ff78c299-1b3f-4d1f-810b-ad9821894e21"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9618' max='9618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9618/9618 3:09:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.068000</td>\n",
              "      <td>0.925363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.056200</td>\n",
              "      <td>0.897019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.021100</td>\n",
              "      <td>0.864732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.964400</td>\n",
              "      <td>0.841201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.945300</td>\n",
              "      <td>0.828586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.916800</td>\n",
              "      <td>0.796897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.871400</td>\n",
              "      <td>0.770540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.844100</td>\n",
              "      <td>0.751579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>0.736352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save model to: /content/drive/MyDrive/Hybrid_Rec_Sys/roberta-base-2024-11-27_04-12-37\n",
            "Training finished\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1030: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='622' max='9618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 622/9618 06:56 < 1:40:40, 1.49 it/s, Epoch 0.19/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for trainer in trainer_list:\n",
        "    trainer.train()\n",
        "    print(\"Save model to:\", output_dir)\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"Training finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uphFSLCCkuun"
      },
      "source": [
        "**Calling trained model to generate embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KswkYAqOhYhD"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/MyDrive/CMSC472 FP/trained_model/bert-base-uncased-2024-11-30_16-38-30\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_yw4B9Sk60b",
        "outputId": "61aec268-e8fc-47f1-dd3c-1bb97b467a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at /content/drive/MyDrive/CMSC472 FP/trained_model/bert-base-uncased-2024-11-30_16-38-30 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModel.from_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the functions\n",
        "def preprocess_news_info(dev_news):\n",
        "    \"\"\"\n",
        "    Preprocess dev_news to filter out news_info entries with raw string length > 512.\n",
        "    \"\"\"\n",
        "    dev_news = dev_news[dev_news['news_info'].str.len() < 512]\n",
        "    return dev_news.reset_index(drop=True)\n",
        "\n",
        "def get_embeddings(texts, tokenizer, model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Get embeddings for a batch of sentences.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
        "    return cls_embeddings\n",
        "\n",
        "def calculate_batch_similarity(text1, batch_texts, tokenizer, model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Calculate similarities between a single text and a batch of other texts.\n",
        "    \"\"\"\n",
        "    embedding1 = get_embeddings([text1], tokenizer, model, device)  # Embedding for the anchor sentence\n",
        "    embedding2 = get_embeddings(batch_texts, tokenizer, model, device)  # Batch embeddings\n",
        "    # Compute cosine similarity between text1 and all batch_texts\n",
        "    similarities = torch.nn.functional.cosine_similarity(embedding1, embedding2).cpu().numpy()\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "8i56Fh4h2B_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_users_by_news = {}\n",
        "user_scores = {}\n",
        "\n",
        "for _, row in train_users.iterrows():\n",
        "    user_id = row[\"user_id\"]\n",
        "    his_list = row[\"history\"]\n",
        "\n",
        "    if user_id not in user_scores:\n",
        "        user_scores[user_id] = 0\n",
        "\n",
        "    for his in his_list:\n",
        "        if his not in group_users_by_news:\n",
        "            group_users_by_news[his] = set()\n",
        "        group_users_by_news[his].add(user_id)"
      ],
      "metadata": {
        "id": "Z9xGszMsCLKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "def get_user_rec(user, tokenizer, model, group_users_by_news, user_scores):\n",
        "\n",
        "  def calculate_scores(user_id_to_set):\n",
        "    for id in user_id_to_set:\n",
        "      user_scores[id] = 1\n",
        "\n",
        "    scores_by_his = {}\n",
        "    for his, user_ids in group_users_by_news.items():\n",
        "        scores_by_his[his] = sum(user_scores[user_id] for user_id in user_ids)\n",
        "\n",
        "    sorted_scores = sorted(scores_by_his.items(), key=lambda x: -x[1])\n",
        "\n",
        "    for id in user_id_to_set:\n",
        "      user_scores[id] = 0\n",
        "\n",
        "    return dict(list(scores_by_his.items())[:5])\n",
        "\n",
        "  def get_score(news):\n",
        "    if news not in group_users_by_news:\n",
        "        return {}\n",
        "    user_ids = group_users_by_news[news]\n",
        "    return calculate_scores(user_ids)\n",
        "\n",
        "\n",
        "  history = user[\"history\"]\n",
        "  top = defaultdict(int)\n",
        "  for his in history:\n",
        "    scores = get_score(his)\n",
        "\n",
        "    for news, score in scores.items():\n",
        "      top[news] += score\n",
        "\n",
        "  sorted_top = dict(sorted(top.items()))\n",
        "  if len(sorted_top) > 10:\n",
        "    return dict(list(sorted_top.items())[:10])\n",
        "  else:\n",
        "    return sorted_top"
      ],
      "metadata": {
        "id": "gfQBvIp02gdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLzqJ8WHvNFV",
        "outputId": "c6f8b2df-5174-46fa-e359-7ea32f56334a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      news_id   category       sub_category  \\\n",
            "0      N55528  lifestyle    lifestyleroyals   \n",
            "1      N19639     health         weightloss   \n",
            "2      N61837       news          newsworld   \n",
            "3      N53526     health             voices   \n",
            "4      N38324     health            medical   \n",
            "...       ...        ...                ...   \n",
            "51277  N16909    weather  weathertopstories   \n",
            "51278  N47585  lifestyle    lifestylefamily   \n",
            "51279   N7482     sports        more_sports   \n",
            "51280  N34418     sports         soccer_epl   \n",
            "51281  N44276      autos        autossports   \n",
            "\n",
            "                                               news_info  \n",
            "0      Category: lifestyle SubCategory: lifestyleroya...  \n",
            "1      Category: health SubCategory: weightloss Label...  \n",
            "2      Category: news SubCategory: newsworld Label1: ...  \n",
            "3      Category: health SubCategory: voices Label1:  ...  \n",
            "4      Category: health SubCategory: medical Label1: ...  \n",
            "...                                                  ...  \n",
            "51277  Category: weather SubCategory: weathertopstori...  \n",
            "51278  Category: lifestyle SubCategory: lifestylefami...  \n",
            "51279  Category: sports SubCategory: more_sports Labe...  \n",
            "51280  Category: sports SubCategory: soccer_epl Label...  \n",
            "51281  Category: autos SubCategory: autossports Label...  \n",
            "\n",
            "[51282 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = train_users.iloc[0]\n",
        "\n",
        "rec = get_user_rec(sample, tokenizer, model, group_users_by_news, user_scores)\n",
        "\n",
        "for news_id in rec.keys():\n",
        "    title = train_news.loc[train_news['news_id'] == news_id, 'news_info'].values\n",
        "\n",
        "    if title:\n",
        "        print(f\"News ID: {news_id}, Title: {title[0]}\")\n",
        "    else:\n",
        "        print(f\"News ID: {news_id} not found in train_news\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vLcCG0ime4I",
        "outputId": "42c19e08-7796-49fb-8542-9d547a651992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News ID: N18445, Title: Category: sports SubCategory: football_ncaa Label1:  Label2:  Title: Michigan sends breakup tweet to Notre Dame as series goes on hold Abstract: Parting is such sweet sorrow, say the Wolverines.\n",
            "News ID: N34694, Title: Category: tv SubCategory: tvnews Label1: Rosie O'Donnell Label2: Rosie O'Donnell Title: Rosie O'Donnell: Barbara Walters Isn't 'Up to Speaking to People' Right Now Abstract: Rosie O'Donnell: Barbara Walters Isn't 'Up to Speaking to People' Right Now\n",
            "News ID: N42782, Title: Category: sports SubCategory: baseball_mlb Label1: New York Yankees Houston Astros Label2: New York Yankees Title: Three takeaways from Yankees' ALCS Game 5 victory over the Astros Abstract: The Yankees kept hope alive thanks to some impressive starting pitching and a pair of early home runs.\n",
            "News ID: N45794, Title: Category: news SubCategory: newscrime Label1:  Label2: Miami International Airport American Airlines Title: Four flight attendants were arrested in Miami's airport after bringing in thousands in cash, police say Abstract: Four American Airlines flight attendants were arrested at the Miami International Airport and charged with money laundering after bringing large amounts of cash into the country, police said.\n",
            "News ID: N55189, Title: Category: tv SubCategory: tvnews Label1:  Label2: Pat Sajak Wheel of Fortune (American game show) Cardiff-by-the-Sea, Encinitas, California California Title: 'Wheel Of Fortune' Guest Delivers Hilarious, Off The Rails Introduction Abstract: We'd like to solve the puzzle, Pat: Blair Davis' loveless marriage? On Monday, \"Wheel of Fortune\" welcomed as a new contestant trucking business owner Blair Davis, who offered a biting introduction for himself. When host Pat Sajak asked the man from Cardiff, California, about his family, Davis plunged into one of the darkest personal summaries the show has likely ever heard. \"I've been trapped in a loveless marriage for the last 12 years to an...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for news_id in sample['history']:\n",
        "    title = train_news.loc[train_news['news_id'] == news_id, 'news_info'].values\n",
        "\n",
        "    if title:\n",
        "        print(f\"News ID: {news_id}, Title: {title[0]}\")\n",
        "    else:\n",
        "        print(f\"News ID: {news_id} not found in train_news\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM6Beg8koFAJ",
        "outputId": "e959632a-20d3-4744-f27f-1f1ddd947949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News ID: N55189, Title: Category: tv SubCategory: tvnews Label1:  Label2: Pat Sajak Wheel of Fortune (American game show) Cardiff-by-the-Sea, Encinitas, California California Title: 'Wheel Of Fortune' Guest Delivers Hilarious, Off The Rails Introduction Abstract: We'd like to solve the puzzle, Pat: Blair Davis' loveless marriage? On Monday, \"Wheel of Fortune\" welcomed as a new contestant trucking business owner Blair Davis, who offered a biting introduction for himself. When host Pat Sajak asked the man from Cardiff, California, about his family, Davis plunged into one of the darkest personal summaries the show has likely ever heard. \"I've been trapped in a loveless marriage for the last 12 years to an...\n",
            "News ID: N42782, Title: Category: sports SubCategory: baseball_mlb Label1: New York Yankees Houston Astros Label2: New York Yankees Title: Three takeaways from Yankees' ALCS Game 5 victory over the Astros Abstract: The Yankees kept hope alive thanks to some impressive starting pitching and a pair of early home runs.\n",
            "News ID: N34694, Title: Category: tv SubCategory: tvnews Label1: Rosie O'Donnell Label2: Rosie O'Donnell Title: Rosie O'Donnell: Barbara Walters Isn't 'Up to Speaking to People' Right Now Abstract: Rosie O'Donnell: Barbara Walters Isn't 'Up to Speaking to People' Right Now\n",
            "News ID: N45794, Title: Category: news SubCategory: newscrime Label1:  Label2: Miami International Airport American Airlines Title: Four flight attendants were arrested in Miami's airport after bringing in thousands in cash, police say Abstract: Four American Airlines flight attendants were arrested at the Miami International Airport and charged with money laundering after bringing large amounts of cash into the country, police said.\n",
            "News ID: N18445, Title: Category: sports SubCategory: football_ncaa Label1:  Label2:  Title: Michigan sends breakup tweet to Notre Dame as series goes on hold Abstract: Parting is such sweet sorrow, say the Wolverines.\n",
            "News ID: N63302, Title: Category: lifestyle SubCategory: lifestylebuzz Label1:  Label2:  Title: This Wedding Photo of a Canine Best Man Captures Just How Deep a Dog's Love Truly Is Abstract: When Mark Doublet made his dog, Marley, the best man at his wedding, he took his duties very seriously, and a photo of them together has gone viral.\n",
            "News ID: N10414, Title: Category: movies SubCategory: movienews Label1: Robert Evans Paramount chief Label2: Robert Evans Urban Cowboy Paramount Pictures Chinatown (1974 film) Title: Robert Evans, 'Chinatown' Producer and Paramount Chief, Dies at 89 Abstract: Robert Evans, the Paramount executive who produced \"Chinatown\" and \"Urban Cowboy\" and whose life became as melodramatic and jaw-dropping as any of his films, died on Saturday night. He was 89. Even though Hollywood history is filled with colorful characters, few can match the tale of Evans, whose life would seem far-fetched if it were\n",
            "News ID: N19347, Title: Category: news SubCategory: newspolitics Label1: Kay Hagan United States Senate Label2: Kay Hagan Greensboro, North Carolina North Carolina United States Senate Title: Former US Senator Kay Hagan dead at 66 Abstract: Former U.S. Sen. Kay Hagan, a one-time Capitol Hill intern who went on to become North Carolina's first Democratic female senator, died Monday at her Greensboro home. She was 66.\n",
            "News ID: N31801, Title: Category: news SubCategory: newspolitics Label1: Joe Biden South Carolina Label2: Catholic Church Joe Biden Title: Joe Biden reportedly denied Communion at a South Carolina church because of his stance on abortion Abstract: Joe Biden has a complicated history with the Catholic Church.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}